import "./DetailedProject.css";
import projects from "../Helpers/ProjectList";
import type { ProjectType } from "../Types/Projects";
import type { ProjectDetails } from "../Types/ProjectDetails";
import { ExternalLink, Github } from "lucide-react";
import { Footer } from "../Components/Footer";
import renderContent from "../Helpers/Render";

function sortProjectsByEnd(projects: ProjectType[]) {
  return [...projects].sort((a, b) => {
    if (!a.yearEnd && !b.yearEnd) return b.year - a.year;
    if (!a.yearEnd) return -1;
    if (!b.yearEnd) return 1;
    return (b.yearEnd ?? 0) - (a.yearEnd ?? 0);
  });
}

function Section({ title, content }: { title: string; content?: string }) {
  if (!content || !content.trim()) return null;

  const className =
    "section " +
    title
      .toLowerCase()
      .replace(/\s+/g, "-")
      .replace(/[^a-z0-9-]/g, "");

  return (
    <section className={className}>
      <h1>{title}</h1>
      <div className="section-body">{renderContent(content.trim())}</div>
    </section>
  );
}

export default function DetailedProjectClass() {
  const sorted = sortProjectsByEnd(projects);

  const found = sorted.find((p) => p.title === "Decomposition Box Plus");

  if (!found) {
    return <p>null</p>;
  }

  const Details: ProjectDetails = {
    title: found.title,
    github: found.github,
    website: found.website,
    oneLiner: "Decomposition tool helping young learners learn problem solving",

    authors: ["Eren Homburg"],
    year: found.year,
    yearEnd: found.yearEnd,
    image: "/ProjectsHeader/DBOX.png",

    abstract: found.abstract,
    problemStatement: `Artificial Intelligence (AI) has rapidly become an essential tool in education, with applications spanning from middle schools to prestigious universities. Students increasingly rely on AI-powered platforms like GitHub Copilot, ChatGPT, Colab, and Coducate to assist with tasks such as algorithm learning and modularizing their code. These tools are praised for their ability to enhance coding efficiency by offering suggestions, auto-completions, and code snippets, helping to streamline the learning process. However, while these tools assist in task completion, they often promote superficial engagement with the material, where students rely on 'copy-pasting' solutions rather than fully understanding the problem and developing critical problem-solving skills. Although they provide a quick way to complete tasks, these tools fail to foster deeper learning and the development of essential cognitive skills like problem decomposition and algorithmic thinking, which are crucial for long-term success in programming education. The central challenge lies in reconciling the convenience of AI tools with the need for meaningful learning. While AI tools offer quick solutions, they often fail to engage students in the active cognitive processes necessary for truly mastering programming. Students who over-rely on these tools for quick fixes miss critical opportunities for problem-solving and critical thinking skills essential for tackling new, unfamiliar problems independently. This over-reliance can lead to a superficial understanding of programming concepts, which hampers the development of deeper cognitive skills like problem decomposition and algorithmic thinking. Cognitive load theory suggests that learning is more effective when learners actively engage with the material rather than passively relying on external aids. Similarly, active learning approaches, which emphasize problem solving and conceptual understanding, have been shown to improve retention and long-term mastery of skills. The opportunity, then, lies in designing AI-powered systems that do more than simply offer solutions. These systems can be structured to encourage learners to engage more deeply with material by guiding them through the problem-solving process and fostering deeper learning and cognitive growth. By embedding AI tools within frameworks that prioritize active learning and cognitive engagement, AI can become a more powerful educational aid, helping students build lasting skills and confidence in their programming abilities. This includes not only algorithmic thinking, but also the ability to structure code in a clear, modular, and maintainable way. By encouraging practices such as abstraction, decomposition, and code reuse, systems like DBox can help scaffold students’ ideas into cleaner implementations that reflect principles such as modularization, decoupling, and the DRY (Don’t Repeat Yourself) principle. These foundational practices not only support better algorithmic thinking but also lay the groundwork for mastering programming paradigms such as object-oriented programming (OOP), where abstraction and modular design play a central role. Following the identified challenges in AI-assisted programming education, we adapted the existing DBox system with a specific focus on supporting structured problem-solving and conceptual understanding. Drawing on scaffolding theory and Vygotsky’s Zone of Proximal Development (ZPD), our design aimed to guide learners just beyond their current capabilities by incrementally supporting higher-order thinking processes. Central to this redesign was the introduction of a second large language model (LLM) to support abstraction as a core cognitive operation. This abstraction layer enables the generation of solutions that are not only correct but also modular, generalized, and cleaner. It helps learners identify patterns, reduce redundancy, and enhance their understanding of code structure and optimization. Rather than offering immediate answers, the enhanced system scaffolds the user’s reasoning process, aligning with cognitive load theory and principles of progressive disclosure to encourage deeper engagement and learning. We ran a small user study with seven participants to investigate how the newly introduced abstraction design performs in practice, identifying limitations and gathering qualitative feedback. Behavioral tracking revealed signs of hesitation and confusion during abstraction tasks, measured through mouse clicks and number of page transitions. The results indicate opportunities to refine the interaction flow and reduce friction, enabling smoother engagement with the system’s learning objectives. The System Usability Scale (SUS) score was 77.14, indicating a strong overall user experience. The average Likert rating was 5.47 out of 7, although usage-frequency scores were lower. This aligns with the participant group, composed of experienced programmers who reported needing less abstraction support for simpler tasks. These findings suggest the importance of designing AI tools that foster deeper engagement and sustained learning for both novice and advanced users. Overall, the evaluation suggests that the redesigned DBox Plus system, with integrated abstraction, offers a more cognitively aligned programming environment conducive to promoting critical thinking and problem-solving skills. This work makes important contributions to the field of human-computer interaction (HCI) and AI-assisted education: {list} 
      1. The AI-assisted system facilitates not only correct code implementations by students but also supports clean, modularized code through guided abstraction and decomposition, promoting deeper understanding and problem-solving in programming. 
      2. A user study provided valuable insights into how users interacted with the abstraction feature, revealing key behaviors such as frustration and confusion points. SUS and Likert scale data confirmed the tool’s usability while highlighting areas for improvement, particularly for more experienced programmers. 
      3. By integrating abstraction into the DBox, this work suggests how AI tools might support the development of higher-order cognitive skills such as abstraction and modularization in algorithmic problem-solving. While direct learning gains were not measured, the system design aligns with cognitive theories that emphasize scaffolding and guided problem decomposition. {/list}
      In conclusion, the contributions presented here suggest new directions and tools for AI-assisted education, emphasizing user-centered design, empirical validation, and theoretical advancement in programming education.`,
    solution: "",
    features: `This work extends Ma et al.’s DBox system. Guided by Human-Centered Design (HCD) principles, we conducted usability testing with both low- and high-fidelity prototypes, iterated on interface designs, and prioritized accessibility improvements based on user feedback. Our high-level design goals were threefold: (1) to scaffold problem decomposition through an intuitive, feedback-rich interface, (2) to elevate abstraction as a core learning activity that fosters generalization and code reuse, and (3) to reduce cognitive overhead via clear structure, minimal visual clutter, and accessible guidance. These goals draw on principles from active learning, cognitive load theory, and user-centered evaluation. Although the original DBox supported decomposition, it lacked abstraction features, an essential skill in modular programming and higher-level problem solving. Our redesign directly addresses this by integrating structured abstraction tools that help students identify and generalize recurring solution patterns. Core Functionalities: This subsection presents the core functionality shared by the original DBox system and our extended version, focusing on elements visible to and directly manipulated by learners. Interface layout, user interactions, design rationale, and technical details are described separately. {img:/DetailedProjectsImg/DBOX/DecompositionInterface.png|alt=DBox Plus Interface|caption=DBox Plus Interface: (A) project file view, (B) problem view, (C) test file, (D) terminal, (E) translating code to step tree button, (F) step's tree, (G) step's interface, (H) step's left group icons, (I) step's right group icons, (J) step's navigation buttons, (K) step's substeps, (L) adding steps, (M) step tree checking button, (N) step's states, (O) step's hints, (P) step tree to code button} Shown in the figure, the interface is divided into three sections: left (file/project navigation), center (problems, step tree, substeps), and right (controls, hints, abstraction tools). The terminal runs locally but supports only limited commands. Step Tree Mechanics: Selected code can be analyzed by the LLM, producing a step tree. Steps are editable, navigable, and supported with icons for editing, duplication, hints, or deletion. Navigation buttons allow hierarchical traversal, and new steps or substeps can be added. Feedback and Hint System: After checking, each step is assigned a state: {list} 
    1. White state: default, not checked. 
    2. Green state: correct. 
    3. Light blue state: correct but further divisible. 
    4. Dashed border: LLM-generated, indicating a missing step. 
    5. Red state: incorrect. 
    6. Dark green state: correct step and implementation after validation. 
    {/list} Hints are also provided: {list} 
    1. General Hint: high-level guidance. 
    2. Detailed Hint: specific clue. 
    3. Reveal Solution: shows full solution. 
    {/list} Abstraction Interface: {img:/DetailedProjectsImg/DBOX/AbstractionInterface.png|alt=Abstraction Interface|caption=Abstraction interface with labeled controls (A–K)} The abstraction interface uses a node-based tree view with three key functions: {list} 
    1. Adding steps: drag and drop into the tree. 
    2. Changing problem: quickly switch tasks. 
    3. Calling abstraction: triggers LLM detection of abstractions (only when tree is fully correct). 
    {/list} After calling, abstraction bubbles appear. Steps can have: {list} 
    1. Grouping state: replaceable with fewer steps. 
    2. Recycling state: reusable sub-step tree. 
    3. Fully abstracted state: cannot be simplified further. 
    {/list} Abstraction overlays allow constructing sub-step trees with hints: {list} 
    1. Step count hint: remaining steps needed. 
    2. Layout hint: reveals intended layout (no more steps can be added). 
    3. Singular step hints: same as in DBox Plus. 
    {/list} Clicking the abstraction check validates the abstraction, replacing original steps. User Interaction: {img:/DetailedProjectsImg/DBOX/UserMoves2.png|alt=User Flow|caption=A typical user flow in the DBox Plus system showing Editor First and Decomposition First paths} Students begin with either Editor First (code directly, then decompose if needed) or Decomposition First (generate a step tree). Feedback and hints are provided iteratively, and successful decomposition leads to implementation and abstraction. Abstractions replace step groups until the tree is fully abstracted, after which abstracted code is implemented. Design Rationale and Feature Evolution: Iterative Refinement of Decomposition included: redesigned step tree (collapsed substeps for scalability), improved hint view (numerical indicators), enhanced micro-interactions (hover effects, contextual arrows, streamlined editing, LLM loading animations), simplification of states and buttons (reducing overload), and added customization (file tree and terminal). {img:/DetailedProjectsImg/DBOX/Comparison.png|alt=Key Interface Changes|caption=Key interface changes: (A) step tree layout, (B) hint indicators, (C) micro-interactions} Extension to Support Abstraction: The abstraction feature identifies five categories: {list} 
    1. Modularizing code via function extraction. 
    2. Merging duplicated logic (DRY). 
    3. Separating input/output concerns. 
    4. Parameterizing behavior for reuse. 
    5. Reintegration of abstracted logic. 
    {/list} LLM detects groupable and recyclable steps, shown via bubble view. Users restructure solutions manually via overlays, with abstraction hints encouraging reflection. Abstraction Limitations: {list} 
    1. Global variables not clearly handled across abstraction groups. 
    2. Bubble spatial layout may mislead by enclosing unrelated steps. 
    3. Conceptual understanding gaps: some participants questioned why abstraction was necessary. 
    {/list} Solutions included tutorials, though many skipped text. Stronger interventions are needed to connect abstraction to real-world practices. Design Trade-offs and Accessibility: Collapsed substeps and simplified states reduce clutter but can hide structure. Flat trees improve clarity but increase scrolling. Visual feedback and color contrasts were tested for accessibility (WCAG 2.1 compliant). Keyboard shortcuts (Shift+Enter, arrows, double-clicks) aid power users. LLM nondeterminism was addressed by reducing call buttons, but still introduces unpredictable feedback. Scaffolding remains static and may be underused. Emphasis on modular decomposition can bias bottom-up strategies. Abstraction adds interactional complexity but preserves learner autonomy, aligning with pedagogy. Together, these trade-offs balance clarity, control, and accessibility while prioritizing pedagogy and extensibility.`,
    walkthrough: "",
    infrastructure:
      "This project was developed with a clear separation between the frontend and backend components, following a modular web architecture. The frontend, responsible for all UI elements, animations, and user interactions, was implemented using React, a component-based JavaScript library, with the entire codebase written in TypeScript for improved type safety and maintainability. The frontend was initially deployed using Wrangler to target Cloudflare Workers, a serverless edge-computing platform. However, to comply with the deployment requirements of the PEACHLAB infrastructure, which relies on Docker-based environments and does not support Cloudflare Workers, an alternative Node.js based version of the frontend was developed. This version enabled containerized deployment while preserving identical behavior and interface logic. {img:/DetailedProjectsImg/DBOX/Deployment.png|alt=Deployment diagram|caption=System deployment architecture Components on the left are hosted within the ETH infrastructure tosatisfy institutional deployment requirements. Components on the right run on Cloudflare Workers, which were independently set up to support scalable and low-latency user interactions.} The backend, also written in TypeScript, is responsible for processing application logic, managing interaction with the LLM, and handling data persistence tasks such as saving, loading, and deleting user-generated content. Like the frontend, the backend was initially deployed using Wrangler as a Cloudflare Worker, but was later refactored to run within a Node.js + Docker environment for infrastructure compatibility. Communication between the frontend and backend is handled via a RESTful API, using standard HTTP methods (GET, POST, DELETE) and structured JSON payloads. In the initial standalone deployment, the backend was hosted on a separate domain (e.g., main-api.com), and API requests were made using fully qualified URLs. In the Docker-based deployment, the backend was co-located with the frontend, and all requests were routed via relative paths prefixed with /api/ (e.g., main/api/save-tree). Despite this change in infrastructure, the system architecture remained stateless and REST-compliant, ensuring modularity and ease of integration across different environments. The system handles user-generated content, such as step trees and abstraction results, using different storage solutions depending on the deployment environment. In the initial Cloudflare Workers deployment, data was temporarily persisted using Cloudflare KV. However, due to its eventual consistency model and size limitations, this was later replaced with R2 buckets, which offer more reliable object storage and better support for structured JSON data. In the ETH infrastructure deployment, data is stored as structured files within directories managed by PEACHLAB. The backend interacts with these files directly through the file system, allowing the application to operate without a dedicated database. This setup satisfies institutional deployment requirements while maintaining consistent application behavior across environments. However, although it is functional and sufficient for the current scope, it is not ideal for long-term scalability, as it lacks the query efficiency and concurrency support provided by a relational database system, making the introduction of a dedicated database layer a clear priority for future development. To support user-level access control, the system implements an authentication mechanism using GitHub OAuth. Users authenticate through their GitHub account, and upon successful login, the backend receives an access token, which is then transformed using a cryptographic hash function to generate a secure session identifier. This hashed token is used to authorize user-specific operations without exposing sensitive credentials, allowing persistent but secure session handling throughout the application lifecycle. The project is configured using Vite, a modern build toolchain that provides rapid development server startup, hot module replacement, and optimized production builds. This setup enabled efficient testing and iteration across both frontend and backend components during development. A range of libraries were used to support development and streamline implementation. For icons, both Font Awesome (FA) and Lucide were integrated. These two libraries provided the full set of icons used throughout the DBox interface, with the exception of the hint lightbulb icon. A suitable lightbulb symbol matching the intended visual style could not be found in either collection. For the embedded code editor, CodeMirror was adopted. It offered a flexible and lightweight interface capable of supporting the IDE-like features required by the system, including syntax highlighting and intelligent code completion. For terminal functionality, Xterm.js was used to render the terminal interface and manage layout structure. All logic related to terminal input, execution, and output formatting was implemented manually. Initial support for code execution relied on a custom backend service accessed through a WebSocket connection deployed on Fly.io. While functionally complete, this architecture introduced operational complexity and potential long-term costs, making it unsuitable for sustained use. As a more sustainable alternative, Pyodide was integrated to enable local execution of Python code directly in the browser. Pyodide compiles the Python interpreter to WebAssembly, which eliminates the need for a backend during code execution. This approach enabled a more seamless and offline-capable experience, though with the tradeoff of slower performance compared to remote execution. At the current stage, the system supports execution of Python code only. The system integrates the OpenAI API, specifically utilizing the gpt-4o model. This version was selected for its advanced reasoning capabilities, lower latency, and improved cost-efficiency compared to earlier models. All requests to the model were made under the user role with a temperature setting of zero, ensuring deterministic and consistent outputs. This deterministic behavior was critical for maintaining structural reliability in the generated code and step tree representations. Inputs to the language model were constructed by combining multiple contextual elements into structured prompts. The templates used to generate these prompts are documented in the appendix. To guide the model effectively in a dynamic, user-driven workflow, the project employed a meta prompting strategy. Meta prompting involves designing higher-level prompts that instruct the model on how to reason, self-verify, or organize its output. This approach was particularly appropriate given the need for interpretable, structured, and self-consistent outputs without continuous user supervision. The expected output from the language model was a JSON-formatted response adhering to a specific schema, depending on the task being performed. These responses typically included either a complete step tree or abstraction suggestions. For abstraction validation tasks, the system used a zero-shot prompting strategy. The model received a structured instruction describing how to compare two step trees based on their hierarchical structure and semantic similarity. These trees were inserted directly into the prompt without any examples. The model was instructed to compare only the content fields of the trees, verifying that each step and substep appeared in the correct order and conveyed equivalent meaning. The expected output was a simple binary response: either Yes or No. Future work may focus on enhancing the model's ability to infer user intent from incomplete or ambiguous inputs. One promising direction is the integration of retrieval-augmented prompts, where relevant context, such as prior user interactions or reference solutions, is retrieved dynamically and embedded into the prompt. This technique has been shown to improve factual grounding and reduce hallucinations in LLMs. Another research avenue involves recursive prompting techniques, where the model iteratively reviews and refines its outputs through self-generated feedback or rule-based constraints. For example, the Self-Refine framework applies a cycle of generation, critique, and revision, improving output quality without requiring additional training data. Similarly, recent work in alignment via refinement (AvR) shows that recursive reasoning techniques can significantly improve performance on multi-step tasks by incorporating self-evaluation into subsequent model passes. Although still an emerging area, these methods suggest that recursive prompting can enable the system to more reliably validate or reconstruct the user’s intended solution tree, ultimately improving overall consistency and accuracy.",
    challenges: "",
    results: `This section presents findings from a user study conducted with seven participants, focusing on the Abstraction component of the DBox Plus. The analysis is structured around three research questions adopted from Ma et al.'s work: {list}
    RQ1: To what extent does DBox Abstraction support students in learning and applying software design principles such as modularization, abstraction, and code reuse?
    RQ2: How does DBox Abstraction affect learners’ perceptions and user experience? 
    RQ3: How do learners interact with DBox Abstraction and perceive the usefulness of its features? 
    {/list} To address these questions, we report on task performance, interaction behaviors, and usability feedback, combining quantitative and qualitative insights. RQ3 is addressed in Task Performance, while RQ1 and RQ2 are covered in Usability and Engagement. Participants completed a sequence of three abstraction tasks using DBox Plus. Performance was assessed in terms of correctness, time spent per phase, total session duration, and the number of interface interactions. All participants completed the protocol in full. Correctness was categorized into three levels: {list}
    Correct: the task was solved using appropriate abstraction principles and yielded a functional solution; 
    Partially Correct: abstraction intent was shown but incomplete or erroneous; 
    Incorrect: no meaningful abstraction and non-functional solution. 
    {/list} Descriptive analysis was used due to the small sample size (N = 7). {img:/DetailedProjectsImg/DBOX/TableDBOX.png|alt=User Study results|caption=Performance metrics across all three phases of the user study} Correctness across participants and phases showed that four participants (P2, P4, P5, P6) completed all three tasks with correct solutions. Phase 1 showed a mix of correct and partial outcomes despite allowing external resources. In Phase 2, correctness remained strong with five correct solutions, though it took the longest on average. Phase 3, without external help, maintained performance with four participants achieving full correctness. One participant remarked, “In my head, I already had an idea of what I wanted to do,” suggesting abstraction planning before implementation. All who achieved correctness in Phase 3, except P6, completed it faster than Phase 1. Total task durations ranged from 1h 25min to just over 2h (mean 1h 51min). Interaction counts ranged from 53 to 127, with no clear relationship to correctness. Metrics such as hint use, overlay toggles, and check-button presses were logged manually. Interaction Behaviors: Figure UserInteraction shows Participant 4’s interaction trace, reflecting common patterns. Behaviors included: {list} 
    (A) Frequent page changes, 
    (B) Overlay toggling and result checking, 
    (C) Repeated hint requests, 
    (D) Step tree referencing, and 
    (E) Use of external resources. 
    {/list} All participants engaged with the step tree, made multiple hint requests, and toggled the overlay more than ten times, showing consistent trial-and-error exploration. Usability and Engagement Feedback: Participants completed the SUS following the session. {img:/DetailedProjectsImg/DBOX/SUSscore.png|alt=SUS Scores|caption=SUS scores across participants} The average SUS score was 77, showing generally positive usability perceptions. Participants also completed a Likert-style questionnaire on learning and usability (QL1–QL4, QR1–QR11). The average rating was 5.47 out of 7. QL1 (Perceived Learning Gain), QL2 (Confidence in Solving Similar Problems), and QR10 (Overall Satisfaction) received high scores, while QR2 and QR11 were more varied. {img:/DetailedProjectsImg/DBOX/Diagram2quest.png|alt=Likert Responses|caption=Responses to Likert-scale questions on engagement and experience} Open-ended responses included positive and critical remarks. Participants praised the hint system but noted visual layout difficulties. One said, “That substep went sideways. Logically, it should have been below one of the three substeps.” These responses are further analyzed in the Discussion section, with full transcripts available in the Appendix. The results suggest that DBox Abstraction offers promising support for learning software design through structured abstraction, as reflected in participants' performance and reported usability. A mean SUS score of 77 and an average Likert rating of 5.47 out of 7 suggest that participants generally found DBox Abstraction both usable and helpful for learning. However, variation in responses, particularly to items of long-term usefulness and relevance of abstraction, indicates that the user experience was influenced by individual expectations and levels of prior experience. Support for Learning Abstraction (RQ1): The performance data across the three abstraction tasks indicates that DBox Abstraction provided meaningful support in helping learners understand and apply software design principles, particularly modularization and abstraction. Four out of seven participants (P2, P4, P5, P6) achieved full correctness across all phases, and most participants succeeded in at least two. Notably, all who reached correctness in the final phase had relied on external help (e.g., ChatGPT) during Phase 1 to complete their solutions, indicating a steep but successful learning curve over the course of the study. From Phase 1 to Phase 3, performance remained stable on average, despite increasing task independence. While external resources were allowed in Phase 1, Phase 2 required abstraction to be applied more independently, which resulted in the longest average task durations. By Phase 3, participants worked without external help or the abstraction tool provided and completed tasks more quickly on average than in Phase 1. These trends suggest that DBox Plus effectively supported procedural fluency, the ability to carry out abstraction tasks with increasing independence and confidence. One participant remarked, “In my head, I already had an idea of what I wanted to do,” suggesting that abstraction strategies had been internalized by the final phase. The same participant also stated, “I already did the thinking part,” reinforcing the idea that the tool helped structure problem-solving early, enabling more independent implementation later. However, some participants continued to experience difficulty in translating abstract ideas into syntactically correct and modular code. This highlights an ongoing challenge in bridging conceptual understanding with implementation. Perceived Usability and Engagement (RQ2): Quantitative feedback, including a mean SUS score of 77 and an average Likert rating of 5.47 across selected items, indicates generally high satisfaction and ease of use. Participants reported positive perceptions of learning gains (QL1), confidence (QL2), and overall satisfaction (QR10). However, qualitative feedback revealed more nuanced experiences. Several participants encountered friction with the tool’s visual structure. One noted, “That substep went sideways. Logically, it should have been below one of the three substeps,” describing a breakdown in hierarchical rendering that affected their mental model of abstraction. Such layout inconsistencies may have contributed to the variability in frustration scores (QR8). Perceptions of long-term usefulness were mixed. Some participants emphasized the tool’s value for scaffolding in early problem-solving but did not anticipate regular use. One stated, “I think I don't want to use it for every task, but maybe for [a] type of task once or twice and then try to solve it without.” Another remarked, “So if I wanted help in a CodeExpert [problem] or something, I could do that. I say it's [rather] helpful for complicated problems. ... If the problem is not so complicated, then I can solve it without it.” These comments suggest that although the tool was perceived as helpful, it may be best suited for complex or unfamiliar tasks, rather than for routine use. Interaction Behaviors and Tool Use (RQ3): The interaction logs revealed consistent engagement across participants, with frequent use of overlays, step tree navigation, and hint requests. All participants requested multiple hints per task and toggled the overlay more than ten times per session. While participants engaged heavily with the tool, their strategies were not always deliberate or consistent. In particular, the frequent toggling of the overlay suggests that users may have struggled to access or retain information efficiently, highlighting a potential need for improved UI continuity and layout clarity. While participants generally understood what was expected at the abstraction level, several struggled to translate those ideas into concrete code. One participant suggested that more integrated syntax support could have helped bridge this gap: “I can also google it, but I like [it when] the concept is presented to you... dictionaries, this, this... or lambda functions as a concept.” This reflects a broader desire for in-tool code examples or inline documentation to support concrete implementation choices without forcing users to switch contexts. Despite this, the consistent use of the step tree across participants suggests that they understood the structural logic of the abstraction process and used the interface effectively to support their reasoning. In Phase 2, the average participant demonstrated smoother navigation, reflected in a reduction of interaction counts during the second task. This pattern indicates that most users adapted quickly to the tool and no longer needed to explore the interface to determine what was required. Limitations: This study has several limitations that should be considered when interpreting the results. The participant group was small (n = 7) and relatively homogeneous, consisting of mostly experienced programmers from similar academic backgrounds. While the tool is intended to support novice learners, the study sample may not fully reflect their needs or behaviors, limiting the generalizability of the findings. Future research should involve a larger and more diverse participant pool to better assess the tool’s impact across different learner profiles. The evaluation was conducted in a remote, semi-controlled setting over Zoom, with each participant completing six tasks across three phases. While this setup allowed for direct observation and screen recording, it also introduced some limitations in terms of experimental control and consistency. The study did not assess long-term retention or transfer of skills beyond the session. Additionally, behavioral data was manually coded from screen recordings, which may have introduced human error or overlooked subtle user interactions. Participants also encountered minor bugs and inconsistencies in the interface, which may have affected engagement and disrupted task flow. One user described their experience as “a little bit [confusing] at the beginning, but in the end it worked,” reflecting friction during initial tool use. Finally, some participants noted that the tool's abstraction scaffolding influenced how they approached problems. One participant reflected, “I partly thought about what was meant instead of how I would solve it,” suggesting that the system's structured guidance may have nudged learners toward predefined solution paths. This highlights a potential trade-off between providing helpful structure and preserving flexibility in student reasoning.`,
    futureWork:
      "This study positions structured abstraction not only as a usability concern, but as a central challenge in how programming knowledge is formed, transferred, and extended. While the immediate findings demonstrate that learners benefit from scaffolded visual reasoning, they also raise broader questions about how abstraction becomes a durable practice, how conceptual thinking connects to implementation, and how future systems might share that reasoning process with learners. Rather than listing specific enhancements, this section outlines three intellectual directions that emerge from these findings and point toward future research in programming cognition, interface design, and human-AI collaboration. Supporting the Development of Durable Abstraction Practices: The study suggests that scaffolded interaction can help learners internalize abstraction strategies, moving from tool-guided behavior toward independent problem solving. This opens up a critical line of inquiry: to what extent can structured interfaces cultivate abstraction practices that persist beyond the immediate learning context? Future work should investigate whether learners transfer these strategies to new domains or programming environments, and how abstraction support can evolve across learning stages. Longitudinal studies, especially with novice participants, are needed to understand how procedural fluency develops and whether tool-mediated learning results in durable conceptual change. Additionally, expanding the participant pool to include true beginners would test the accessibility and adaptability of DBox Plus across levels of prior knowledge and cognitive readiness. Bridging Conceptual Reasoning and Code Execution: Several participants experienced friction when translating abstract designs into functional code, particularly in the absence of concrete examples. This gap highlights a familiar challenge in programming education: helping learners connect high-level concepts with the actual syntax needed to implement them. Future research should explore hybrid interfaces that bridge this divide, such as context-sensitive syntax suggestions, inline code scaffolds, or interactive prompts that model common abstraction patterns. Beyond usability, this direction touches on deeper cognitive questions: can the design of the interface itself reduce context switching and reinforce abstraction as a cognitive habit rather than just a structural convention? Exploring these design challenges could inform not only DBox, but a broader class of educational tools that aim to align thinking and doing in computational problem solving. Rethinking AI's Role in Novice Programming Environments: Finally, this work points toward a reimagined role for artificial intelligence in early-stage programming. Rather than positioning AI as a generator of solutions or provider of corrections, DBox Plus suggests a model in which AI contributes to the abstraction process as a reflective collaborator. Future versions of the system could incorporate retrieval-augmented prompting to ground suggestions in prior interactions, or use recursive prompting to iteratively refine student-generated structures. Techniques like Self-Refine and alignment via refinement (AvR) offer promising foundations for such approaches. These methods could help the system more reliably reconstruct user intent and offer multiple viable abstractions, encouraging exploration and metacognitive reflection. Investigating this direction requires both advances in prompt engineering and a deeper understanding of how learners perceive AI-generated scaffolds, not simply as automation but as shared reasoning partners. This redefines what tutoring can look like by moving from explanation alone to interactive models where learners and systems co-construct understanding through shared agency. Integrating Directions: Together, these three research directions emphasize a shift in how programming environments can support learning, not by simplifying code, but by enriching the cognitive and social scaffolds around it. This includes fostering durable abstraction habits, helping learners connect conceptual reasoning to code, and positioning AI as a reflective partner. Each direction contributes to a broader vision of tools that support deeper thinking, not just more efficient coding. Advancing this agenda will require interdisciplinary work across HCI, cognitive science, education, and AI, offering promise in the design of more human-centered programming systems.",
    tags: found.tags,
  };

  return (
    <>
      <div className="details-container">
        <div className="title-projectContainer">
          {Details.image && (
            <div
              className="title-right-projectContainer dboxpic"
              style={{ backgroundImage: `url(${Details.image})` }}
            />
          )}
          <div className="title-left-projectContainer">
            <div className="topPart">
              <h1>{Details.title}</h1>

              {Details.oneLiner && (
                <p className="oneLiner">"{Details.oneLiner}"</p>
              )}
            </div>
            <div className="authorsAndYear">
              {Details.authors.map((author, i) => (
                <span key={i}>
                  {author === "Eren Homburg" ? <b>{author}</b> : author}
                  {i < Details.authors.length - 1 ? ", " : ""}
                </span>
              ))}
              <p style={{ margin: 0 }}>
                {Details.yearEnd
                  ? Details.year === Details.yearEnd
                    ? Details.year
                    : `${Details.year} - ${Details.yearEnd}`
                  : `${Details.year} - Present`}
              </p>
            </div>
            <div className="tagsAndLinks">
              {/* Tags */}
              <div className="tags detailed">
                {Details.tags.map((tag, i) => (
                  <span key={`${tag}-${i}`} className="tag">
                    {tag}
                  </span>
                ))}
              </div>
              <div className="links detailed">
                {Details.github && (
                  <a
                    href={Details.github}
                    target="_blank"
                    rel="noopener noreferrer"
                  >
                    <Github className="link-projectDetails" />
                    Github
                  </a>
                )}
                {Details.website && (
                  <a
                    href={Details.website}
                    target="_blank"
                    rel="noopener noreferrer"
                  >
                    <ExternalLink className="link-projectDetails" />
                    External Link
                  </a>
                )}
              </div>
            </div>
          </div>
        </div>
        <div className="main-detailedProject">
          {/* Sections from Abstract down */}
          <Section title="Abstract" content={Details.abstract} />
          <Section
            title="Problem Statement"
            content={Details.problemStatement}
          />
          <Section title="Solution" content={Details.solution} />
          <Section title="Features" content={Details.features} />
          <Section title="Walkthrough" content={Details.walkthrough} />
          <Section title="Infrastructure" content={Details.infrastructure} />
          <Section title="Challenges" content={Details.challenges} />
          <Section title="Results" content={Details.results} />
          <Section title="Future Work" content={Details.futureWork} />
        </div>
      </div>
      <Footer />
    </>
  );
}
